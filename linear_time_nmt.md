## Neural Machine Translation in Linear Time 

**Authors**: *Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, AÃ¤ron van den Oord, Alex Graves, Koray Kavukcuoglu*, "Neural Machine Translation in Linear Time"

**One-liner**: The authors provide a neural architecture for sequence processing (i.e. language modelling, machine translation) with two important properties, specifically that it runs in linear time and preserves temporal resolution, rivaling and outperforming recurrent neural networks.

### Problem Setting

In **neural language modeling**, we would like our model to estimate the distribution of the sequences of words or characters that belong to the given language.

<p align="center">
 <img src="/img/nmt_linear/lang_model.gif" width="260px">
</p>

On the other hand, in **neural machine translation**, we would like to estimate the distribution of the sequence in the target language *conditioned* on a given sequence in the source language.

<p align="center">
 <img src="/img/nmt_linear/transl_model.gif" width="300px">
</p>

The current state of the art in these tasks is achieved with a powerful type of sequence model called Recurrent Neural Networks (and their variants such as LSTMs and GRUs).

### Drawbacks of RNN's

1. Cannot be run in parallel along the sequence length.
2. Hard to learn long-term dependencies.
3. Gradient explosion/vanishing problem (mitigated with LSTM's).
4. Source sequence is of constant size in translation architectures.

### Advantages of ByteNet

1. Linear runtime - up to a constant
2. Parallelizable training and decoding.
3. Resolution preserving - the size of the representation generated by the source network is proportional to the amount of information it represents and predicts later on.
4. Dependencies over large distances are connected by short paths.

### ByteNet Model Overview

The target network is stacked on the source network.

<p align="center">
 <img src="/img/nmt_linear/arch_representation.png" width="300px">
</p>

**Source network** uses:

- 1-D dilated convolutions
- Convolutions are not masked

**Target network** uses:

- Dynamic unfolding (to generate variable length output sequences)
- 1-D dilated convolutions
- Masked convolutions

Note that the ByteNet Decoder is the target network of ByteNet. All in all, the model looks like the following:

<p align="center">
 <img src="/img/nmt_linear/architecture.png" width="350px">
</p>

### Architecture Details

**Dynamic Unfolding.** The goal of dynamic unfolding is to accomodate source and target sequences of different lengths. To do this, the source network starts by generating a string representation that has the same length as the source sequence. Then, anything beyond the length of the source sequence is zero-padded.

<p align="center">
 <img src="/img/nmt_linear/d_unfolding.png" width="350px">
</p>

**Masked 1D Convolutions.** Sort of like the causal property described in the Wavenet paper and the masked convolutions described in the PixelCNN paper as well. Basically, the masking ensures that information from future tokens does not affect the prediction of the current token.

**Dilation.** This increases the receptive field of the target network exponentially. The dilation scheme is double at every layer and capped at 16.

**Residual Blocks.** 2 variants of residual blocks are wrapped around each layer. ReLU for machine translation and Multiplicative Units for language modeling.

<p align="center">
 <img src="/img/nmt_linear/residual.png" width="350px">
</p>

**Sub-Batch Normalization.** A variant of BN is developped so as to not violate the probability equation written above. Batch is devided into main and auxiliary, mean and variance computer over auxiliary and used to normalize layer using main batch. Loss is computed on main-batch only.

**Word Embedding.** Bag of n-gram character is used only for translation task.

### Parameter Specifics

The character-level language model is evaluated on the Hutter Prize version of Wikipedia. The parameters are as follows:

- 25 residual blocks split into 5 sets of 5 blocks each.
- Dilations rate are 1, 2, 4, 8 and 16.
- Masked kernel size of 3.
- Number of hidden units d = 892.
- Residual multiplicative blocks.
- Sub-batch normalization.
- Adam with learning rate of 10^-2 and decay of 10^-5.
- batch size is 515 characters, 315 of which are context and 200 are predicted.

The character-level machine translation model is evaluated on the WMT English to German dataset. The parameters are as follows:

- 15 residual blocks in the source and 15 in the target, arranged in sets of 5.
- Dilation rates of 1, 2, 4, 8 and 16.
- Residual blocks with ReLUs and Sub-BN.
- Number of hidden units d = 892.
- Kernel size is 1 x 5 for for source, and masked 1 x 3 for target.
- Bag of character n-grams embedding for source and target inputs.
- Adam with learning rate of 0.003.
- Sentence is padded with special characters to the nearest greater multiple of 25.
- Each pair of sentences is mapped to a bucked based on length.
- During decoding, use beam search to search over possible target intervals.

 
